{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept - ML with Homomorphic Encryption \n",
    "\n",
    "Basic idea is to try out very simple neural network in Torch to be converted into an equivalent network that processes homomorphic encrpyted data. \n",
    "In this example, we are using two libraries: TenSEAL by OpenMined and Concrete-ML by Zama. \n",
    "\n",
    "**Dataset used**: Titanic\n",
    "\n",
    "**Model architecture**:\n",
    "- Linear (256, 15) // 256 neurons, 15 input features\n",
    "- ReLU \n",
    "- Linear (2, 256)  // 2 output class\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "Version Log and Todo:\n",
    "\n",
    "- Need to recheck on Tenseal context to perform multiple matrix multiplications\n",
    "---\n",
    "\n",
    "This Proof of Concept is largely influenced by \n",
    "\n",
    "https://github.com/OpenMined/TenSEAL/blob/main/tutorials/Tutorial%201%20-%20Training%20and%20Evaluation%20of%20Logistic%20Regression%20on%20Encrypted%20Data.ipynb#:~:text=Tutorial,-1%20%2D%20Training%20and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tenseal as ts\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# those are optional and are not necessary for training\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".data/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        \n",
    "        cdf = df.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\n",
    "        # bucketing\n",
    "        cdf['Age'] = pd.cut(cdf['Age'], bins=[0, 18, 40, 60, 100],\n",
    "                            labels=['child', 'young', 'middle', 'old'])\n",
    "\n",
    "        # separate Cabin values\n",
    "        def preprocess_cabin(x):\n",
    "            if pd.isnull(x):\n",
    "                return pd.Series([np.nan, np.nan, np.nan], \n",
    "                                index=['Number_of_rooms', 'Cabin_Deck', 'Cabin_Room'])\n",
    "            else:\n",
    "                rooms = x.split(' ')\n",
    "                return pd.Series([len(rooms), rooms[0][0], rooms[0][1:]], \n",
    "                                index=['Number_of_rooms', 'Cabin_Deck', 'Cabin_Room'])\n",
    "\n",
    "        # Apply preprocess_cabin and create new columns\n",
    "        # cabin_features = cdf['Cabin'].apply(preprocess_cabin)\n",
    "        # cdf[['Number_of_rooms', 'Cabin_Deck', 'Cabin_Room']] = cabin_features\n",
    "\n",
    "        # one hot encoding\n",
    "        cdf['Pclass'] = cdf['Pclass'].apply(\n",
    "            lambda x: {1: 'upper', 2: 'middle', 3: 'lower'}[x])\n",
    "        cdf = pd.get_dummies(\n",
    "            cdf, columns=['Age', 'Embarked', 'Pclass', 'Sex'])\n",
    "\n",
    "        cdf = cdf.drop(['Cabin',], axis=1)\n",
    "\n",
    "        # First, separate numeric and boolean columns\n",
    "        numeric_cols = ['SibSp', 'Parch', 'Fare']\n",
    "        bool_cols = [col for col in cdf.columns if cdf[col].dtype == 'bool']\n",
    "\n",
    "        # For numeric columns, use StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        cdf[numeric_cols] = scaler.fit_transform(cdf[numeric_cols])\n",
    "\n",
    "        # Boolean columns are already 0 and 1, no need to normalize\n",
    "        # But convert to float for PyTorch\n",
    "        for col in bool_cols:\n",
    "            cdf[col] = cdf[col].astype('float32')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if train:    \n",
    "            self.data = cdf.drop('Survived', axis=1).values.astype('float32')\n",
    "            self.target = cdf['Survived'].values\n",
    "        else:\n",
    "            self.data = cdf.values.astype('float32')\n",
    "            self.target = None\n",
    "        self.nfeatures = self.data.shape[1]\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.data[idx], self.target[idx]\n",
    "        else:\n",
    "            return self.data[idx], np.nan\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(df)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(pd.read_csv(\".data/titanic/test.csv\"), train=False)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(), # skip ReLU to make life a little bit easier for now\n",
    "                # but apparently, Zama's Concrete won't accept Linear combined together without activation? so turning this on\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for mps / cuda as its a very simple network\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleClassifier(dataset.nfeatures).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "  loss: 0.5248889327049255\n",
      "  accuracy: 0.6734006734006734\n",
      "epoch: 2\n",
      "  loss: 0.48460689187049866\n",
      "  accuracy: 0.7867564534231201\n",
      "epoch: 3\n",
      "  loss: 0.5914096832275391\n",
      "  accuracy: 0.8103254769921436\n",
      "epoch: 4\n",
      "  loss: 0.37213996052742004\n",
      "  accuracy: 0.8080808080808081\n",
      "epoch: 5\n",
      "  loss: 0.2948571443557739\n",
      "  accuracy: 0.813692480359147\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    classifier.train()\n",
    "    \n",
    "    for i, (data, target) in enumerate((dataloader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = classifier(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = output.argmax(dim=1)\n",
    "        running_correct += (pred == target).sum().item()\n",
    "        running_total += target.size(0)\n",
    "    \n",
    "    print(f\"epoch: {epoch+1}\")\n",
    "    print(f\"  loss: {loss.item()}\")\n",
    "    print(f\"  accuracy: {running_correct / running_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concrete ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<concrete.ml.quantization.quantized_module.QuantizedModule at 0x318424be0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "import numpy\n",
    "\n",
    "torch_input = torch.randn(100, 15)\n",
    "quantized_module = compile_torch_model(\n",
    "    classifier.to('cpu'), # our model\n",
    "    torch_input, # a representative input-set to be used for both quantization and compilation\n",
    "    n_bits=6,\n",
    "    rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"}\n",
    ")\n",
    "\n",
    "quantized_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5436, -4608]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = next(iter(testloader))\n",
    "x_test_q = quantized_module.quantize_input(data.numpy())\n",
    "y_pred = quantized_module.quantized_forward(x_test_q, fhe=\"execute\")\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00673574, -0.78111645]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = quantized_module.dequantize_output(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9749, -0.7431]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still exploring to see how we could \"examine\" the encryption on Concrete (i.e. how we see the encrypted X and Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (data, target) in enumerate(testloader):\n",
    "#     y_pred = quantized_module.forward(data.numpy(), fhe=\"execute\")\n",
    "#     print(y_pred)\n",
    "#     break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tenseal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 15])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model[0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tenseal Context \n",
    "\n",
    "# parameters\n",
    "poly_mod_degree = 16384\n",
    "coeff_mod_bit_sizes = [60, 40, 40, 60]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 40\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# poly_mod_degree = 16384  # Increased for more precision\n",
    "# coeff_mod_bit_sizes = [50, 30, 30, 30, 50]  # More conservative chain\n",
    "# scale = 2 ** 30  # Matched with middle layers\n",
    "\n",
    "# # create TenSEALContext\n",
    "# ctx_eval = ts.context(\n",
    "#     scheme=ts.SCHEME_TYPE.CKKS,\n",
    "#     poly_modulus_degree=poly_mod_degree,\n",
    "#     coeff_mod_bit_sizes=coeff_mod_bit_sizes\n",
    "# )\n",
    "# ctx_eval.global_scale = scale\n",
    "# ctx_eval.generate_galois_keys()\n",
    "\n",
    "# ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree = 16384, coeff_mod_bit_sizes = [60, 60, 60, 60, 60, 60])\n",
    "# ctx_eval.generate_galois_keys()\n",
    "# ctx_eval.global_scale = 2**25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.8791, 3.8791]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing \n",
    "a = torch.rand((1, 5))\n",
    "b = torch.ones((5, 2))\n",
    "\n",
    "enc_a = ts.ckks_tensor(ctx_eval, a)\n",
    "enc_b = ts.ckks_tensor(ctx_eval, b)\n",
    "\n",
    "(a @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.8792, 3.8792]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((enc_a @ enc_b).decrypt().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tenseal.tensors.ckkstensor.CKKSTensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.8792, 3.8792]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(enc_a))\n",
    "print(type(b))\n",
    "torch.tensor((enc_a @ b).decrypt().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4995, -0.4002, -0.4978,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "<class 'tenseal.tensors.ckkstensor.CKKSTensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tenseal.tensors.ckkstensor.CKKSTensor at 0x31c860e20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, _ = next(iter(testloader))\n",
    "enc_x = ts.ckks_tensor(ctx_eval, x.tolist())\n",
    "\n",
    "print(x)\n",
    "print(type(enc_x))\n",
    "print(type(classifier.model[0].weight.data))\n",
    "enc_x @ (classifier.model[0].weight.data.cpu().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CKKSTensor.scale of <tenseal.tensors.ckkstensor.CKKSTensor object at 0x31c817e80>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_x.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate a forward pass - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = classifier.model[0].weight.data.cpu().T\n",
    "w_.to(torch.float8_e4m3fn)\n",
    "xl = enc_x @ (classifier.model[0].weight.data.cpu().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tenseal.tensors.ckkstensor.CKKSTensor at 0x31c84fa90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stuck at this point, scale out of bounds\n",
    "\n",
    "w2_ = classifier.model[2].weight.data.cpu().T\n",
    "w2_.to(torch.float8_e5m2)\n",
    "\n",
    "xl @ (w2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in this cell and after did not consider RELU yet, so need to adapt (hardcoded 2 layers)\n",
    "class EncryptedClassifier:\n",
    "    \n",
    "    def __init__(self, unencrypted_model):\n",
    "        \n",
    "        # assuming same architecture\n",
    "        self.w1 = unencrypted_model.model[0].weight.data.cpu() # [256, 15]\n",
    "        self.b1 = unencrypted_model.model[0].bias.data.cpu()  # [256]\n",
    "        \n",
    "        self.w2 = unencrypted_model.model[2].weight.data.cpu()  # [2, 256]\n",
    "        self.b2 = unencrypted_model.model[2].bias.data.cpu()  # [2]\n",
    "        \n",
    "\n",
    "    # def forward(self, enc_x):\n",
    "    #     # assume encrypted input\n",
    "        \n",
    "    #     # enc_x would be in the shape of [1, 15]\n",
    "    #     # while w1 is in shape [256, 15] \n",
    "    #     results = [ ]\n",
    "    #     for row in self.w1.T:\n",
    "    #         part_result = enc_x @ row  # Dot product instead of full matrix mult\n",
    "    #         results.append(part_result) # a little tricky here since the individual items are in form ckks_tensor, how to stack them (?)\n",
    "        \n",
    "    #     # Combine results\n",
    "    #     x = torch.stack(results)\n",
    "    #     x = enc_x @ self.w1.T \n",
    "    #     x = x @ self.w2.T \n",
    "        \n",
    "    #     return x\n",
    " \n",
    " \n",
    "    ## apparently, the context need to be adjusted so we can do matmul twice.\n",
    "    ## it keeps making scale out of bounds error. To be researched\n",
    "    def forward(self, enc_x):\n",
    "        # assume encrypted input\n",
    "        \n",
    "        # enc_x would be in the shape of [1, 15]\n",
    "        # while w1 is in shape [256, 15] \n",
    "        \n",
    "        x = enc_x @ self.w1.T \n",
    "        # need relu, but not sure how\n",
    "        x = x @ self.w2.T \n",
    "        \n",
    "        return x        \n",
    "        \n",
    "        \n",
    "        # enc_z = []\n",
    "        \n",
    "        # # calculate dot product of each neuron\n",
    "        # for i in range(self.w1.shape[0]):               # 256 times\n",
    "        #     # print(f\"forward 1 - {i}\")\n",
    "        #     z = enc_x.dot(self.w1[i, :])    # dot product input x neuron weights\n",
    "        #     z += self.b1[i]                 # add bias value \n",
    "        #     enc_z.append(z)         # add the result\n",
    "            \n",
    "        # # enc_z should be an array of [256, 1]\n",
    "        # print(type(enc_z[0]))\n",
    "        # print(type(enc_z))\n",
    "        \n",
    "        # # redo for second layer\n",
    "        # enc_z2 = []\n",
    "        # # calculate dot product of each neuron\n",
    "        # for i in range(self.w2.shape[0]):   # 2 times\n",
    "        #     print(f\"forward 2 - {i}\")\n",
    "        #     z = enc_z.dot(self.w2[i, :])    # dot product input x neuron weights\n",
    "        #     z += self.b2[i]                 # add bias value \n",
    "        #     enc_z2.append(enc_z2)         # add the result\n",
    "                  \n",
    "        # # enc_z should be an array of [2, 1]\n",
    "        # return enc_z\n",
    "\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "        \n",
    "\n",
    "    \n",
    "    def encrypt(self, context):\n",
    "        self.w1 = ts.ckks_vector(context, self.w1)\n",
    "        self.b1 = ts.ckks_vector(context, self.b1)\n",
    "        \n",
    "        \n",
    "        self.w2 = ts.ckks_vector(context, self.w2)\n",
    "        self.b2 = ts.ckks_vector(context, self.b2)\n",
    "        \n",
    "    def decrypt(self, context):\n",
    "        self.w1 = self.w1.decrypt()\n",
    "        self.b1 = self.b1.decrypt()\n",
    "        \n",
    "        self.w2 = self.w2.decrypt()\n",
    "        self.b2 = self.b2.decrypt()\n",
    "        \n",
    "\n",
    "encCls = EncryptedClassifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 0\n",
      "\t input tensor: tensor([-0.4995, -0.4002, -0.4978,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "         1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000])\n",
      "\t ckks enc tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x3c6f2a590>\n",
      "\t forward output tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x3c6f29990>\n",
      "\t decrypted output: tensor([[ 1.2319, -1.2118]])\n",
      "Instance 1\n",
      "\t input tensor: tensor([ 0.6170, -0.4002, -0.5127,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000])\n",
      "\t ckks enc tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x17899ead0>\n",
      "\t forward output tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x3c6f2a140>\n",
      "\t decrypted output: tensor([[-0.3896,  0.3020]])\n",
      "Instance 2\n",
      "\t input tensor: tensor([-0.4995, -0.4002, -0.4645,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "         1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000])\n",
      "\t ckks enc tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x17899f3a0>\n",
      "\t forward output tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x31c8c1f60>\n",
      "\t decrypted output: tensor([[ 1.1485, -0.6670]])\n",
      "Instance 3\n",
      "\t input tensor: tensor([-0.4995, -0.4002, -0.4829,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000])\n",
      "\t ckks enc tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x31c8c3190>\n",
      "\t forward output tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x3c6f2aad0>\n",
      "\t decrypted output: tensor([[ 1.2461, -1.1561]])\n",
      "Instance 4\n",
      "\t input tensor: tensor([ 0.6170,  0.6199, -0.4180,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000])\n",
      "\t ckks enc tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x31c8c3520>\n",
      "\t forward output tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x3c6f2b0d0>\n",
      "\t decrypted output: tensor([[-0.8163,  0.5331]])\n",
      "Instance 5\n",
      "\t input tensor: tensor([-0.4995, -0.4002, -0.4728,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000])\n",
      "\t ckks enc tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x17899f3a0>\n",
      "\t forward output tensor: <tenseal.tensors.ckkstensor.CKKSTensor object at 0x3c6f2a140>\n",
      "\t decrypted output: tensor([[ 0.9163, -0.9686]])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate((testloader)):\n",
    "    print(f\"Instance {i}\")\n",
    "    \n",
    "    x = data[0]\n",
    "    print(f\"\\t input tensor: {x}\")\n",
    "    \n",
    "    enc_x = ts.ckks_tensor(ctx_eval, [x])\n",
    "    print(f\"\\t ckks enc tensor: {enc_x}\")\n",
    "    \n",
    "    # encrypted evaluation\n",
    "    enc_out = encCls(enc_x)\n",
    "    print(f\"\\t forward output tensor: {enc_out}\")\n",
    "    \n",
    "    # plain comparison\n",
    "    out = enc_out.decrypt()\n",
    "    print(f\"\\t decrypted output: {torch.tensor(out.tolist())}\")\n",
    "    \n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
